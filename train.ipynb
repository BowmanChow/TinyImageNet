{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.0 64-bit ('tf': conda)",
   "display_name": "Python 3.7.0 64-bit ('tf': conda)",
   "metadata": {
    "interpreter": {
     "hash": "ef6fde6de5204c40dbcdcc5a1d628b0cb82bf9ba4127c180e5260673b8b417bc"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import torch.utils.data as data\n",
    "import matplotlib.image as pli\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "file_path = './TinyImageNet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageSet(data.Dataset):\n",
    "    def __init__(self):\n",
    "        self.length = 100 * 1000\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # print(index)\n",
    "        label = int(index / 1000)\n",
    "        img_count = index % 1000\n",
    "        # print(label)\n",
    "        # print(img_count)\n",
    "        img = Image.open(f'train/{label}/{label}_{img_count}.jpg')\n",
    "        # img.show()\n",
    "        img = my_transform(img)\n",
    "        # print(img.size())\n",
    "        # exit(0)\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_loader = data.DataLoader(ImageSet(), batch_size=256, shuffle=True)\n",
    "test_loader = data.DataLoader(ImageSet(), batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ConvNet(\n  (layer1): Sequential(\n    (0): Conv2d(3, 8, kernel_size=(1, 1), stride=(1, 1))\n    (1): ReLU()\n    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (layer2): Sequential(\n    (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1))\n    (1): ReLU()\n    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (layer3): Sequential(\n    (0): Conv2d(16, 32, kernel_size=(4, 4), stride=(1, 1))\n    (1): ReLU()\n    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (layer4): Sequential(\n    (0): Linear(in_features=1152, out_features=600, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=600, out_features=100, bias=True)\n  )\n)\n"
     ]
    }
   ],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            # 64\n",
    "            nn.Conv2d(in_channels=3, out_channels=8, kernel_size=1),\n",
    "            # 64\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, padding=0)\n",
    "            # 32\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            # 32\n",
    "            nn.Conv2d(in_channels=8, out_channels=16,\n",
    "                      kernel_size=3),\n",
    "            # 30\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, padding=0)\n",
    "            # 15\n",
    "        )\n",
    "        self.layer3 = nn.Sequential(\n",
    "            # 15\n",
    "            nn.Conv2d(in_channels=16, out_channels=32,\n",
    "                      kernel_size=4),\n",
    "            # 12\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, padding=0)\n",
    "            # 6\n",
    "        )\n",
    "        self.layer4 = nn.Sequential(\n",
    "            # 6 \n",
    "            nn.Linear(in_features=1152, out_features=600),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=600, out_features=100)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        out = self.layer1(input)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.layer4(out)\n",
    "        return out\n",
    "\n",
    "convNet = ConvNet()\n",
    "print(convNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "ConvNet(\n",
       "  (layer1): Sequential(\n",
       "    (0): Conv2d(3, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Conv2d(16, 32, kernel_size=(4, 4), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Linear(in_features=1152, out_features=600, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=600, out_features=100, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "# 加载模型， 请谨慎操作， 会覆盖在内存中的模型\n",
    "convNet = ConvNet()\n",
    "convNet.load_state_dict(torch.load('./ConvNet.model'))\n",
    "convNet.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(convNet.parameters(), lr=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "i = 0,  loss = 4.204866409301758,  accuracy = 0.0546875\n",
      "i = 50,  loss = 4.171463489532471,  accuracy = 0.05859375\n",
      "i = 100,  loss = 4.23508358001709,  accuracy = 0.06640625\n",
      "i = 150,  loss = 4.2233734130859375,  accuracy = 0.06640625\n",
      "i = 200,  loss = 4.140136241912842,  accuracy = 0.06640625\n",
      "i = 250,  loss = 4.195827007293701,  accuracy = 0.05078125\n",
      "i = 300,  loss = 4.077423572540283,  accuracy = 0.0859375\n",
      "i = 350,  loss = 4.214171886444092,  accuracy = 0.078125\n"
     ]
    }
   ],
   "source": [
    "for i, (images, labels) in enumerate(train_loader):\n",
    "    outputs = convNet.forward(images)\n",
    "    loss = loss_func(outputs, labels)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    predict = torch.argmax(nn.functional.softmax(outputs, dim=1), dim=1)\n",
    "    if i % 50 == 0:\n",
    "        print(f\"i = {i},  loss = {loss},  accuracy = {sum(labels == predict).item() / labels.size(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型， 请谨慎操作， 会覆盖文件中的模型\n",
    "torch.save(convNet.state_dict(), './ConvNet.model')"
   ]
  }
 ]
}