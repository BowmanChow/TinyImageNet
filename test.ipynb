{"metadata":{"language_info":{"name":"python","version":"3.8.5-final","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"name":"Python 3.8.5 64-bit","display_name":"Python 3.8.5 64-bit","metadata":{"interpreter":{"hash":"31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"}}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":["from torch.autograd import Variable\n","import torch.nn.functional as F\n","import torch.nn as nn\n","import torch\n","import torchvision\n","from torchvision import datasets, transforms\n","import torch.utils.data as data\n","import torchvision.models as models\n","import matplotlib.image as pli\n","import matplotlib.pyplot as plt\n","import matplotlib as mpl\n","import numpy as np\n","from PIL import Image"],"metadata":{"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["my_transform = transforms.Compose(\n","    [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n","\n","file_path = './TinyImageNet'"],"metadata":{"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["label_num = 100\n","\n","class ImageSet(data.Dataset):\n","    def __init__(self):\n","        self.length = 10000\n","\n","    def __getitem__(self, index):\n","        img = Image.open(f'{file_path}/test/{index}.jpg')\n","        # img.show()\n","        img = my_transform(img)\n","        # print(img.size())\n","        # print(aaa)\n","        return index, img\n","\n","    def __len__(self):\n","        return self.length"],"metadata":{"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["test_loader = data.DataLoader(ImageSet(), batch_size=100, shuffle=False)"],"metadata":{"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# 加载模型， 请谨慎操作， 会覆盖在内存中的模型\n","convNet = models.resnet18()\n","convNet.conv1 = nn.Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","convNet.maxpool = nn.Sequential()\n","convNet.layer4 = nn.Sequential()\n","convNet.fc = nn.Linear(256,label_num)\n","convNet.load_state_dict(torch.load('./ConvNet.model'))\n","convNet.eval()"],"metadata":{"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"ResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): Sequential()\n  (layer1): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer4): Sequential()\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Linear(in_features=256, out_features=100, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":["convNet.eval()\n","device = torch.device(\"cuda\")\n","convNet = convNet.to(device)\n","index_list = torch.zeros((0)).int()\n","index_list = index_list.to(device)\n","predict_list = torch.zeros((0)).int()\n","predict_list = predict_list.to(device)\n","for i, (index, images) in enumerate(test_loader):\n","    index = index.to(device)\n","    images = images.to(device)\n","    outputs = convNet(images)\n","    predict = torch.argmax(F.softmax(outputs, dim=1), dim=1)\n","    print(f\"i = {i}, index = {index[0]}:{index[-1]} predict = {predict[0]}:{predict[-1]}\")\n","    index_list = torch.cat((index_list,index.int()))\n","    predict_list = torch.cat((predict_list,predict.int()))\n","    # print(index_list)\n","    # print(predict_list)"],"metadata":{"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"i = 0, index = 0:99 predict = 84:22\ni = 1, index = 100:199 predict = 95:14\ni = 2, index = 200:299 predict = 68:75\ni = 3, index = 300:399 predict = 78:61\ni = 4, index = 400:499 predict = 36:28\ni = 5, index = 500:599 predict = 40:96\ni = 6, index = 600:699 predict = 35:80\ni = 7, index = 700:799 predict = 62:6\ni = 8, index = 800:899 predict = 3:99\ni = 9, index = 900:999 predict = 73:44\ni = 10, index = 1000:1099 predict = 92:38\ni = 11, index = 1100:1199 predict = 14:57\ni = 12, index = 1200:1299 predict = 81:95\ni = 13, index = 1300:1399 predict = 78:54\ni = 14, index = 1400:1499 predict = 25:7\ni = 15, index = 1500:1599 predict = 79:57\ni = 16, index = 1600:1699 predict = 14:73\ni = 17, index = 1700:1799 predict = 32:79\ni = 18, index = 1800:1899 predict = 21:38\ni = 19, index = 1900:1999 predict = 80:40\ni = 20, index = 2000:2099 predict = 53:76\ni = 21, index = 2100:2199 predict = 93:89\ni = 22, index = 2200:2299 predict = 60:52\ni = 23, index = 2300:2399 predict = 28:45\ni = 24, index = 2400:2499 predict = 0:72\ni = 25, index = 2500:2599 predict = 86:73\ni = 26, index = 2600:2699 predict = 49:99\ni = 27, index = 2700:2799 predict = 80:61\ni = 28, index = 2800:2899 predict = 6:89\ni = 29, index = 2900:2999 predict = 14:9\ni = 30, index = 3000:3099 predict = 81:69\ni = 31, index = 3100:3199 predict = 64:86\ni = 32, index = 3200:3299 predict = 17:45\ni = 33, index = 3300:3399 predict = 17:72\ni = 34, index = 3400:3499 predict = 56:13\ni = 35, index = 3500:3599 predict = 94:41\ni = 36, index = 3600:3699 predict = 67:36\ni = 37, index = 3700:3799 predict = 46:62\ni = 38, index = 3800:3899 predict = 5:48\ni = 39, index = 3900:3999 predict = 62:94\ni = 40, index = 4000:4099 predict = 79:89\ni = 41, index = 4100:4199 predict = 96:7\ni = 42, index = 4200:4299 predict = 22:17\ni = 43, index = 4300:4399 predict = 7:91\ni = 44, index = 4400:4499 predict = 41:73\ni = 45, index = 4500:4599 predict = 81:19\ni = 46, index = 4600:4699 predict = 96:4\ni = 47, index = 4700:4799 predict = 89:71\ni = 48, index = 4800:4899 predict = 32:47\ni = 49, index = 4900:4999 predict = 6:26\ni = 50, index = 5000:5099 predict = 12:87\ni = 51, index = 5100:5199 predict = 88:22\ni = 52, index = 5200:5299 predict = 88:88\ni = 53, index = 5300:5399 predict = 56:99\ni = 54, index = 5400:5499 predict = 96:23\ni = 55, index = 5500:5599 predict = 57:96\ni = 56, index = 5600:5699 predict = 3:88\ni = 57, index = 5700:5799 predict = 0:86\ni = 58, index = 5800:5899 predict = 94:19\ni = 59, index = 5900:5999 predict = 72:68\ni = 60, index = 6000:6099 predict = 73:43\ni = 61, index = 6100:6199 predict = 88:36\ni = 62, index = 6200:6299 predict = 93:93\ni = 63, index = 6300:6399 predict = 91:88\ni = 64, index = 6400:6499 predict = 88:57\ni = 65, index = 6500:6599 predict = 14:57\ni = 66, index = 6600:6699 predict = 66:13\ni = 67, index = 6700:6799 predict = 80:79\ni = 68, index = 6800:6899 predict = 88:76\ni = 69, index = 6900:6999 predict = 83:46\ni = 70, index = 7000:7099 predict = 46:88\ni = 71, index = 7100:7199 predict = 0:6\ni = 72, index = 7200:7299 predict = 64:81\ni = 73, index = 7300:7399 predict = 88:92\ni = 74, index = 7400:7499 predict = 28:14\ni = 75, index = 7500:7599 predict = 81:36\ni = 76, index = 7600:7699 predict = 99:4\ni = 77, index = 7700:7799 predict = 42:92\ni = 78, index = 7800:7899 predict = 92:36\ni = 79, index = 7900:7999 predict = 83:88\ni = 80, index = 8000:8099 predict = 0:95\ni = 81, index = 8100:8199 predict = 93:66\ni = 82, index = 8200:8299 predict = 36:46\ni = 83, index = 8300:8399 predict = 47:79\ni = 84, index = 8400:8499 predict = 68:65\ni = 85, index = 8500:8599 predict = 96:84\ni = 86, index = 8600:8699 predict = 84:13\ni = 87, index = 8700:8799 predict = 88:88\ni = 88, index = 8800:8899 predict = 88:46\ni = 89, index = 8900:8999 predict = 55:36\ni = 90, index = 9000:9099 predict = 79:88\ni = 91, index = 9100:9199 predict = 0:47\ni = 92, index = 9200:9299 predict = 73:73\ni = 93, index = 9300:9399 predict = 78:73\ni = 94, index = 9400:9499 predict = 72:95\ni = 95, index = 9500:9599 predict = 80:87\ni = 96, index = 9600:9699 predict = 91:0\ni = 97, index = 9700:9799 predict = 57:98\ni = 98, index = 9800:9899 predict = 44:88\ni = 99, index = 9900:9999 predict = 9:35\n","output_type":"stream"}]},{"cell_type":"code","source":["import pandas as pd\n","img_list = [f'{i}.jpg' for i in index_list.cpu()]\n","dataframe = pd.DataFrame({'Id':img_list,'Category':predict_list.cpu()})\n","dataframe.to_csv(\"test.csv\",index=False,sep=',')"],"metadata":{"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["import moxing as mox\n","mox.file.copy_parallel(\"test.csv\",\"obs://deep-learning-hw2-zzzzzzjjjbbb/test.csv\")"],"metadata":{"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":[],"metadata":{},"execution_count":null,"outputs":[]}]}